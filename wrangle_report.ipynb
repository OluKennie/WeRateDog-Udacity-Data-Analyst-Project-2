{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting: wrangle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document briefly describes the wrangling efforts for the WeRateDogs Twitter dataset in the wrangle_act.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was gather through:\n",
    "1. File on hand \n",
    "````twitter_archive_enhanced.csv````\n",
    "2. File hosted on Udacity server\n",
    "````image_predictions.tsv````\n",
    "3. Query of Twitter API \n",
    "````tweet_json.txt````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter_archive_enhanced.csv was read directly into a DataFrame using the pandas library, .csv files.\n",
    "image_predictions.tsv\n",
    "Files hosted on the internet were programmatically downloaded using the requests library. After the download, the pandas library was used to read in the .tsv into a DataFrame.\n",
    "tweet_json.txt\n",
    "I used the tweet_json.txt file provided by udacity, because I could not get the necessary secret Keys from Twitter developers dashboard, as my request to sign up is yet to be approved. The text file is read line by line to append the tweet_id, favorite_count, and retweet_count into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data were assessed both visually and programmatically to look for quality and tidiness issues.\n",
    "Programmatic Methods:\n",
    "data.head()\n",
    "data.describe()\n",
    "data.info()\n",
    "data.duplicated()\n",
    "data.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidiness issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The tweet_archive and image_prediction can be joined into one dataset\n",
    "\n",
    "2. Redundant columns of the same category, which is now divided into many columns, but only one stage column is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The dataset contains some retweets, these would be removed.\n",
    "\n",
    "2. The source column will have to be changed from ulr type to text.\n",
    "\n",
    "3. Non-dog names should be converted to 'None'.\n",
    "\n",
    "4. Tweets without images will be removed.\n",
    "\n",
    "5. Unnecessary columns will be removed.\n",
    "\n",
    "6. The timestamp will be converted to datetime\n",
    "\n",
    "7. There are empty values in several columns\n",
    "\n",
    "8. Some of the rating_numerator and rating_denominator have offbeat values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the cleaning process performed on the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness issue\n",
    "1. Simplify 3 tables to 2 by joining (inner) archived_tweet_copy with img_dataframe_copy to create one tweet observation table.\n",
    "2. Convert doggo, flooter, pupper, puppo columns into one stage column in the tweet_combined, then drop the four columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality issue\n",
    "1. Retweets rows and  'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp' columns were removed.\n",
    "2. The source was Stripped to remove the HTML link.\n",
    "3. Dog names were replaced with 'None', and lowercase dog names were also replaced. Names that were not found were replaced with not a number(NaN).\n",
    "4. Rows where all predictions of dog breed is not a dog were dropped.\n",
    "5. I removed the jpg_url, in_reply_to_status_id, in_reply_to_user_id and expanded_urls  column from img_dataframe and archived_tweet dataframe. \n",
    "6. Timestamp were converted to datetime.\n",
    "7. Rows with missing values were removed. \n",
    "8. Variable types were change to appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gathered, assessed, and cleaned master dataset were saved to a CSV file named \"twitter_archive_master.csv\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
